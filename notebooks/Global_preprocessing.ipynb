{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febad66d-0fac-4c6e-b5db-ce2e5af0fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google in /home/diana/.pyenv/versions/3.10.6/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/diana/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from google) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/diana/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from beautifulsoup4->google) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67121eb1-05e2-46fa-8213-f54b1297700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from langdetect import detect, LangDetectException\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from unidecode import unidecode\n",
    "import io\n",
    "\n",
    "\n",
    "# Load the English language model for Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def filter_and_update_categories(df):\n",
    "\n",
    "    main_cat_counts = df['Main Category'].value_counts()\n",
    "    freq_nouns = Counter()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        doc = nlp(row['Processed Text'])\n",
    "        nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "        freq_nouns.update(nouns)\n",
    "\n",
    "    df['Main Category'] = df['Main Category'].apply(\n",
    "        lambda x: freq_nouns.most_common(1)[0][0] if main_cat_counts[x] < 10 else x\n",
    "    )\n",
    "\n",
    "    all_cats = Counter([cat for sublist in df['All Categories'].str.split(',').dropna() for cat in sublist])\n",
    "    df['All Categories'] = df['All Categories'].str.split(',').apply(\n",
    "        lambda cats: [cat.strip() for cat in cats if all_cats[cat.strip()] >= 10] if cats else []\n",
    "    )\n",
    "\n",
    "\n",
    "    df['All Categories'] = df['All Categories'].apply(\n",
    "        lambda x: [word for word, count in freq_nouns.most_common(3)] if not x else x\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_top_words(text, n=10):\n",
    "    \"\"\"Return the most common words in the text.\"\"\"\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    most_common_words = Counter(words).most_common(n)\n",
    "    return most_common_words\n",
    "\n",
    "def standardize_dates(df, date_column):\n",
    "    \"\"\"Standardizes the dates in a DataFrame column to the format 'YYYY-MM-DD'.\"\"\"\n",
    "    df[date_column] = df[date_column].apply(lambda date: parser.parse(date).strftime('%Y-%m-%d'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Processes the given text by cleaning and normalizing it.\"\"\"\n",
    "    text = text.lower().strip()  # Convert text to lowercase and strip whitespaces\n",
    "    text = unidecode(text)  # Normalize text\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple spaces into one\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes the given text, removing stopwords, very short words, and certain parts of speech.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and len(token.lemma_) > 2 and token.lemma_.isalpha()\n",
    "        and token.pos_ not in ['PRON', 'DET']\n",
    "    ]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "def detect_english(text):\n",
    "    \"\"\"Detects if the given text is English.\"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def filter_english_text(df, text_column):\n",
    "    \"\"\"Filters the DataFrame to only include rows where the text is English.\"\"\"\n",
    "    df['is_english'] = df[text_column].apply(detect_english)\n",
    "    filtered_df = df[df['is_english']].drop(columns=['is_english'])\n",
    "    return filtered_df\n",
    "\n",
    "def get_most_frequent_word(text):\n",
    "    \"\"\"Extracts the most frequent word from the text.\"\"\"\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        return Counter(words).most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def get_most_frequent_bigram(text):\n",
    "    \"\"\"Extracts the most frequent two-word combination from the text.\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) > 1:\n",
    "        bigrams = zip(words[:-1], words[1:])\n",
    "        return ' '.join(Counter(bigrams).most_common(1)[0][0])\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6faefe37-ecec-4ac2-8979-2aae69513622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_gcp(bucket_name, file_name):\n",
    "    # Create a storage client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    # Get the blob (file) from the bucket\n",
    "    blob = bucket.get_blob(file_name)\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(io.BytesIO(blob.download_as_bytes()), sep=\",\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09919b-54e4-45d7-a6db-7799c2d3a3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135dca5-da80-478b-8e5c-973291de42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "def plot_word_count_distribution(df, column, title):\n",
    "    \"\"\"Plot the word count distribution of a text column.\"\"\"\n",
    "    word_counts = df[column].str.split().str.len()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(word_counts, bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Word Count Distribution in {title}')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_word_count_distribution(combined_df, 'Main Body', 'Original Texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d028b-130c-4f8f-bf86-6f572b9b48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Further clean the text by removing numbers and non-alphanumeric characters, and converting to lowercase.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "combined_df['Cleaned Body'] = combined_df['Main Body'].apply(clean_text)\n",
    "\n",
    "all_cleaned_texts = ' '.join(combined_df['Cleaned Body'])\n",
    "top_words = get_top_words(all_cleaned_texts, 10)\n",
    "\n",
    "print(\"Top 10 Most Frequent Words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31f12c-251a-4667-b761-0180544081e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(df, text_column):\n",
    "    \"\"\"Applies text cleaning, English filtering, and updates most frequent words in a DataFrame.\"\"\"\n",
    "    df = df.dropna(subset=[text_column])\n",
    "    df = filter_english_text(df, text_column)\n",
    "    # df = standardize_dates(df, date_column)\n",
    "\n",
    "    if not df.empty:\n",
    "        df['Processed Text'] = df[text_column].apply(preprocess_text)\n",
    "        df = filter_and_update_categories(df)\n",
    "        df = lemmatize_text(df)\n",
    "        df['Most Frequent Word'] = df['Processed Text'].apply(get_most_frequent_word)\n",
    "        df['Most Frequent Word Combination'] = df['Processed Text'].apply(get_most_frequent_bigram)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2a4c8-068e-4389-9f6d-90c0dcf00658",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = preprocessing_pipeline(combined_df, 'Main Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db54c79-79bc-4bcc-a5c7-4325f3cd0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_count_distribution(updated_df, 'Processed Text', 'Cleaned Texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a6180-39aa-481f-94e3-0790f0f5f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Further clean the text by removing numbers and non-alphanumeric characters, and converting to lowercase.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "all_cleaned_texts = ' '.join(updated_df['Processed Text'])\n",
    "top_words = get_top_words(all_cleaned_texts, 10)\n",
    "\n",
    "print(\"Top 10 Most Frequent Words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717acc46-6081-42b5-bbe5-07a6acbee12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for the next step of embedding in the pipeline'''\n",
    "# from gensim.models import FastText\n",
    "# import nltk\n",
    "# import numpy as np\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# def tokenize_texts(text_list):\n",
    "#     return [nltk.word_tokenize(text) for text in text_list]\n",
    "\n",
    "# def create_fasttext_model(tokenized_texts, size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "#     model = FastText(tokenized_texts, size=size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "#     return model\n",
    "\n",
    "# def prepare_word_vectors(model):\n",
    "#     return {word: model.wv[word] for word in model.wv.vocab}\n",
    "\n",
    "# class MeanEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec.get(w, np.zeros(self.dim)) for w in words], axis=0)\n",
    "#             for words in X\n",
    "#         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171bebf-fb49-4138-9e0f-8f30bdce57ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 (pyenv)",
   "language": "python",
   "name": "pyenv-3.10.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
